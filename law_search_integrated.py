import asyncio
import os
import time
import re
from typing import List, Dict, Any
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from tavily import TavilyClient
from dotenv import load_dotenv


# LLMìš©
from openai import OpenAI

# ë¡œì»¬ ëª¨ë“ˆ import
from law_article_extractor import (
    extract_law_articles,
    extract_all_articles_with_references,
    extract_referenced_articles,
)
from law_content_fetcher import LawContentFetcher

load_dotenv()


class LawSearchIntegrated:
    """í†µí•© ë²•ë ¹ ê²€ìƒ‰ ë° ì¡°ë¬¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° í´ë˜ìŠ¤"""

    def __init__(self):
        self.tavily_api_key = os.getenv("TAVILY_API_KEY")
        self.google_cse_api_key = os.getenv("GOOGLE_CSE_API_KEY")
        self.google_cse_engine_id = os.getenv("GOOGLE_CSE_ENGINE_ID")
        self.law_fetcher = LawContentFetcher()
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.openai_model = os.getenv("OPENAI_MODEL")
        if self.openai_api_key:
            self.openai_client = OpenAI(api_key=self.openai_api_key)
        else:
            self.openai_client = None

    def extract_keywords(self, query: str) -> str:
        """ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (í˜„ì¬ëŠ” ì›ë³¸ ì§ˆë¬¸ ë°˜í™˜)"""
        # TODO: í–¥í›„ kiwipiepy ë“± í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ì¶”ê°€ ì˜ˆì •
        return query

    def _select_best_law_name(self, query: str, laws: List[Dict[str, str]]) -> str:
        """ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë²•ë ¹ëª…ì„ ì„ íƒ"""
        if not laws:
            return ""

        # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        query_keywords = set(query.replace(" ", "").lower())

        best_law = laws[0]  # ê¸°ë³¸ê°’
        best_score = 0

        for law in laws:
            law_name = law["law_name"]
            score = 0

            # 1. ì§ˆë¬¸ì— ë²•ë ¹ëª…ì´ ì§ì ‘ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if law_name in query:
                score += 10

            # 2. ë²•ë ¹ëª…ì˜ í‚¤ì›Œë“œê°€ ì§ˆë¬¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            law_keywords = set(law_name.replace(" ", "").lower())
            common_keywords = query_keywords.intersection(law_keywords)
            score += len(common_keywords) * 2

            # 3. ë²•ë ¹ëª…ì´ ì§§ê³  ëª…í™•í•œì§€ í™•ì¸ (ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™ë³´ë‹¤ëŠ” ê¸°ë³¸ ë²•ë ¹ ì„ í˜¸)
            if "ì‹œí–‰ë ¹" not in law_name and "ì‹œí–‰ê·œì¹™" not in law_name:
                score += 3
            elif "ì‹œí–‰ë ¹" in law_name:
                score += 1

            # 4. ë²•ë ¹ëª…ì´ ë„ˆë¬´ ê¸¸ì§€ ì•Šì€ì§€ í™•ì¸
            if len(law_name) <= 10:
                score += 2

            # 5. íŠ¹ì • íŒ¨í„´ ì œì™¸ (ì˜ëª»ëœ ì¶”ì¶œ ë°©ì§€)
            if any(
                bad_pattern in law_name
                for bad_pattern in ["ì…ì°°ìëŠ”", "ê°™ì€ ë²•", "ì´ ë²•"]
            ):
                score -= 5

            if score > best_score:
                best_score = score
                best_law = law

        return best_law["law_name"]

    def tavily_search(
        self, query: str, domains: List[str] | None = None, num_results: int = 5
    ) -> List[str]:
        """Tavily APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if not self.tavily_api_key:
                print("âš ï¸  TAVILY_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []

            client = TavilyClient(api_key=self.tavily_api_key)

            # ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì„¤ì •
            search_params = {
                "query": query,
                "search_depth": "basic",
                "max_results": num_results,
                "include_answer": False,
                "include_raw_content": False,
                "include_images": False,
            }

            # ë„ë©”ì¸ ì§€ì •ì´ ìˆìœ¼ë©´ ì¶”ê°€
            if domains:
                search_params["include_domains"] = domains
                print(f"ğŸ” ì§€ì •ëœ ë„ë©”ì¸ì—ì„œ ê²€ìƒ‰: {', '.join(domains)}")

            response = client.search(**search_params)

            urls = []
            if "results" in response:
                for result in response["results"]:
                    if "url" in result:
                        urls.append(result["url"])

            return urls

        except Exception as e:
            print(f"Tavily ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

    def google_cse_search(
        self, query: str, domains: List[str] | None = None, num_results: int = 5
    ) -> List[str]:
        """Google Custom Search Engine APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if not self.google_cse_api_key or not self.google_cse_engine_id:
                print(
                    "âš ï¸  GOOGLE_CSE_API_KEY ë˜ëŠ” GOOGLE_CSE_ENGINE_ID í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                )
                return []

            import requests
            from urllib.parse import quote_plus

            # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            search_query = query
            if domains:
                domain_filter = " ".join([f"site:{domain}" for domain in domains])
                search_query = f"{query} {domain_filter}"
                print(f"ğŸ” Google CSE ê²€ìƒ‰ - ì§€ì •ëœ ë„ë©”ì¸: {', '.join(domains)}")

            # Google CSE API URL êµ¬ì„±
            base_url = "https://www.googleapis.com/customsearch/v1"
            params = {
                "key": self.google_cse_api_key,
                "cx": self.google_cse_engine_id,
                "q": search_query,
                "num": min(num_results, 10),  # Google CSEëŠ” ìµœëŒ€ 10ê°œ ê²°ê³¼
            }

            response = requests.get(base_url, params=params, timeout=10)
            response.raise_for_status()

            data = response.json()
            urls = []

            if "items" in data:
                for item in data["items"]:
                    if "link" in item:
                        urls.append(item["link"])
                        if len(urls) >= num_results:
                            break

            print(f"ğŸ” Google CSE ê²€ìƒ‰ìœ¼ë¡œ {len(urls)}ê°œ URL ë°œê²¬")
            return urls

        except Exception as e:
            print(f"Google CSE ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

    def search_urls(
        self,
        original_query: str,
        extracted_query: str,
        domains: List[str] | None = None,
        num_results: int = 5,
    ) -> List[str]:
        """Google CSEëŠ” ì›ë³¸ ì¿¼ë¦¬, TavilyëŠ” ì¶”ì¶œëœ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰"""
        # Google CSE ìš°ì„ 
        if self.google_cse_api_key and self.google_cse_engine_id:
            print("ğŸ” Google CSE APIë¡œ ê²€ìƒ‰ ì¤‘... (ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©)")
            urls = self.google_cse_search(original_query, domains, num_results)
            if urls:
                return urls
            else:
                print("âš ï¸  Google CSE ê²€ìƒ‰ ì‹¤íŒ¨, Tavily APIë¡œ fallback...")

        # Tavily fallback
        if self.tavily_api_key:
            print("ğŸ” Tavily APIë¡œ ê²€ìƒ‰ ì¤‘... (ì¶”ì¶œëœ í‚¤ì›Œë“œ ì‚¬ìš©)")
            urls = self.tavily_search(extracted_query, domains, num_results)
            if urls:
                return urls
            else:
                print("âš ï¸  Tavily ê²€ìƒ‰ ì‹¤íŒ¨")

        raise ValueError(
            "ê²€ìƒ‰ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GOOGLE_CSE_API_KEYì™€ GOOGLE_CSE_ENGINE_ID ë˜ëŠ” TAVILY_API_KEY ì¤‘ í•˜ë‚˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
        )

    def clean_markdown_text(self, text: str) -> str:
        """ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ì—ì„œ URL ë§í¬ ì œê±°"""
        if not text:
            return text

        # ë§ˆí¬ë‹¤ìš´ ë§í¬ íŒ¨í„´ ì œê±°: [í…ìŠ¤íŠ¸](URL) -> í…ìŠ¤íŠ¸
        text = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", text)

        # ì¼ë°˜ URL íŒ¨í„´ ì œê±°: http://... ë˜ëŠ” https://...
        text = re.sub(r"https?://[^\s\)\]\>]+", "", text)

        # ì´ë¯¸ì§€ ë§í¬ ì œê±°: ![alt](URL)
        text = re.sub(r"!\[([^\]]*)\]\([^)]+\)", "", text)

        # ë¹ˆ ì¤„ ì •ë¦¬
        text = re.sub(r"\n\s*\n\s*\n", "\n\n", text)

        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()

        return text

    async def crawl_and_extract_laws(
        self, query: str, domains: List[str] | None = None, num_results: int = 5
    ) -> Dict[str, Any]:
        """ê²€ìƒ‰ â†’ í¬ë¡¤ë§ â†’ ë²•ë ¹ ì¶”ì¶œ â†’ ì¡°ë¬¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° + LLM ë‹µë³€"""

        print(f"ğŸ” ê²€ìƒ‰ ì‹œì‘: '{query}'")

        # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
        search_query = self.extract_keywords(query)

        # 2. Google CSE ë˜ëŠ” Tavily APIë¡œ URL ìˆ˜ì§‘
        try:
            urls = self.search_urls(query, search_query, domains, num_results)
        except ValueError as e:
            return {
                "success": False,
                "error": str(e),
                "search_query": query,
                "crawled_content": "",
                "extracted_laws": [],
                "law_contents": [],
                "llm_answer": None,
            }

        if not urls:
            return {
                "success": False,
                "error": "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API í‚¤ì™€ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.",
                "search_query": query,
                "crawled_content": "",
                "extracted_laws": [],
                "law_contents": [],
                "llm_answer": None,
            }

        # ë‹¤ìš´ë¡œë“œ ë§í¬ í•„í„°ë§
        filtered_urls = []
        download_patterns = [
            "flDownload.do",
            "downloadGet",
            "download.filespec",
            "download.savedname",
            "download.filename",
            ".hwp",
            ".pdf",
            ".doc",
            ".docx",
            ".xls",
            ".xlsx",
            ".ppt",
            ".pptx",
            ".zip",
            ".rar",
            ".exe",
            ".msi",
            ".dmg",
            ".pkg",
        ]

        for url in urls:
            is_download = any(pattern in url.lower() for pattern in download_patterns)
            if not is_download:
                filtered_urls.append(url)
            else:
                print(f"ğŸš« ë‹¤ìš´ë¡œë“œ ë§í¬ ì œì™¸: {url}")

        if not filtered_urls:
            return {
                "success": False,
                "error": "í¬ë¡¤ë§ ê°€ëŠ¥í•œ URLì´ ì—†ìŠµë‹ˆë‹¤.",
                "search_query": query,
                "crawled_content": "",
                "extracted_laws": [],
                "law_contents": [],
                "llm_answer": None,
            }

        print(f"ğŸ“„ {len(filtered_urls)}ê°œì˜ URLì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")

        # 2. í¬ë¡¤ë§í•˜ì—¬ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        all_text = ""

        # ë§í¬ ì œê±°ë¥¼ ìœ„í•œ CrawlerRunConfig ì„¤ì •
        config = CrawlerRunConfig(
            exclude_external_links=True,
            exclude_internal_links=True,
            exclude_social_media_links=True,
            exclude_all_images=True,
        )

        # Crawl4AI ë¡œê·¸ ì¶œë ¥ ì–µì œ
        import logging
        import sys

        # ëª¨ë“  ë¡œê·¸ ë ˆë²¨ì„ ERRORë¡œ ì„¤ì •
        logging.getLogger().setLevel(logging.ERROR)
        logging.getLogger("crawl4ai").setLevel(logging.ERROR)
        logging.getLogger("urllib3").setLevel(logging.ERROR)
        logging.getLogger("requests").setLevel(logging.ERROR)

        SUPPRESS_STDOUT = False
        if SUPPRESS_STDOUT:
            # í‘œì¤€ ì¶œë ¥ ë¦¬ë‹¤ì´ë ‰ì…˜ (ì„ì‹œ)
            original_stdout = sys.stdout
            sys.stdout = open(os.devnull, "w")

        async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:
            for i, url in enumerate(filtered_urls, 1):
                try:
                    print(f"í¬ë¡¤ë§ ì¤‘ ({i}/{len(filtered_urls)}): {url}")
                    result = await crawler.arun(url=url, config=config)

                    # result ì²˜ë¦¬ - CrawlResultContainer._results ë‚´ë¶€ ì ‘ê·¼
                    try:
                        markdown_content = None
                        # _results ì•ˆì „ ì ‘ê·¼
                        results_list = None
                        if isinstance(result, dict) and "_results" in result:
                            results_list = result["_results"]
                        else:
                            results_list = getattr(result, "_results", None)
                        if results_list and isinstance(results_list, list):
                            for item in results_list:
                                item_dict = item
                                if not isinstance(item, dict) and hasattr(
                                    item, "__dict__"
                                ):
                                    item_dict = item.__dict__
                                if isinstance(item_dict, dict):
                                    for key in ["markdown", "content", "text"]:
                                        if key in item_dict and item_dict[key]:
                                            markdown_content = item_dict[key]
                                            break
                                if not markdown_content:
                                    for key in ["markdown", "content", "text"]:
                                        val = getattr(item, key, None)
                                        if val:
                                            markdown_content = val
                                            break
                                if markdown_content:
                                    break
                        if markdown_content:
                            cleaned_text = self.clean_markdown_text(markdown_content)
                            all_text += f"\n\n--- {url} ---\n\n{cleaned_text}"
                            # print(f"DEBUG: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ, ê¸¸ì´: {len(cleaned_text)}")
                        else:
                            print(f"í¬ë¡¤ë§ ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}")
                    except Exception as e:
                        print(f"ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue

                except Exception as e:
                    print(f"í¬ë¡¤ë§ ì‹¤íŒ¨ ({url}): {e}")
                    continue

                # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                time.sleep(1)

        if SUPPRESS_STDOUT:
            # í‘œì¤€ ì¶œë ¥ ë³µì›
            sys.stdout.close()
            sys.stdout = original_stdout

        if not all_text.strip():
            return {
                "success": False,
                "error": "í¬ë¡¤ë§ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.",
                "search_query": query,
                "crawled_content": "",
                "extracted_laws": [],
                "law_contents": [],
                "llm_answer": None,
            }

        print(f"ğŸ“ í¬ë¡¤ë§ ì™„ë£Œ: {len(all_text)} ë¬¸ì")

        # 3. ë²•ë ¹ëª…ê³¼ ì¡°ë¬¸ë²ˆí˜¸ ì¶”ì¶œ (ì§ì ‘ ì–¸ê¸‰ + ì°¸ì¡°)
        print("ğŸ” ë²•ë ¹ëª…ê³¼ ì¡°ë¬¸ë²ˆí˜¸ ì¶”ì¶œ ì¤‘...")

        # ë²•ë ¹ëª…ê³¼ ì¡°ë¬¸ë²ˆí˜¸ ì¶”ì¶œ
        initial_laws = extract_law_articles(all_text)
        current_law_name = None

        if initial_laws:
            # ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë²•ë ¹ì„ ê¸°ì¤€ìœ¼ë¡œ ì„ íƒ
            current_law_name = self._select_best_law_name(query, initial_laws)
            print(f"ğŸ“‹ ê¸°ì¤€ ë²•ë ¹: {current_law_name}")

            # ë””ë²„ê·¸: ì¶”ì¶œëœ ëª¨ë“  ë²•ë ¹ ì¶œë ¥
            print(f"ğŸ” ì¶”ì¶œëœ ë²•ë ¹ë“¤:")
            for i, law in enumerate(initial_laws, 1):
                print(f"  {i}. {law['law_name']} ì œ{law['article_num']}ì¡°")

        # ì§ì ‘ ì–¸ê¸‰ëœ ì¡°í•­ê³¼ ì°¸ì¡° ì¡°í•­ ëª¨ë‘ ì¶”ì¶œ
        all_extracted = extract_all_articles_with_references(all_text, current_law_name)
        extracted_laws = all_extracted["all_articles"]
        direct_laws = all_extracted["direct_articles"]
        referenced_laws = all_extracted["referenced_articles"]

        print(
            f"ğŸ“‹ ì¶”ì¶œëœ ë²•ë ¹: {len(extracted_laws)}ê°œ (ì§ì ‘: {len(direct_laws)}ê°œ, ì°¸ì¡°: {len(referenced_laws)}ê°œ)"
        )
        for i, law in enumerate(direct_laws, 1):
            print(f"  {i}. {law['law_name']} ì œ{law['article_num']}ì¡° (ì§ì ‘ ì–¸ê¸‰)")
        for i, law in enumerate(referenced_laws, 1):
            print(
                f"  {len(direct_laws) + i}. {law['law_name']} ì œ{law['article_num']}ì¡° (ì°¸ì¡°)"
            )

        # 4. ì¶”ì¶œëœ ë²•ë ¹ì˜ ì¡°ë¬¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        law_contents = []
        if extracted_laws:
            print("ğŸ“– ì¡°ë¬¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì¤‘...")
            # print(f"DEBUG: extracted_laws: {extracted_laws}")
            law_contents = await self.law_fetcher.fetch_law_articles_content(
                extracted_laws
            )

            # 5. ì¡°ë¬¸ ë‚´ìš©ì—ì„œ ì¶”ê°€ ì°¸ì¡° ì¡°í•­ ì¶”ì¶œ
            additional_references = []
            for content_result in law_contents:
                if (
                    content_result.get("content", {}).get("success")
                    and current_law_name
                ):
                    content_text = content_result["content"]["content"].get(
                        "content", ""
                    )
                    if content_text:
                        # ì¡°ë¬¸ ë‚´ìš©ì—ì„œ ì°¸ì¡° ì¶”ì¶œ
                        content_refs = extract_referenced_articles(
                            content_text, current_law_name
                        )
                        additional_references.extend(content_refs)

            # ì¤‘ë³µ ì œê±°
            unique_additional_refs = []
            seen_keys = set()
            for ref in additional_references:
                if ref["key"] not in seen_keys:
                    unique_additional_refs.append(ref)
                    seen_keys.add(ref["key"])

            if unique_additional_refs:
                print(
                    f"ğŸ“‹ ì¡°ë¬¸ ë‚´ìš©ì—ì„œ ì¶”ê°€ ì°¸ì¡° ë°œê²¬: {len(unique_additional_refs)}ê°œ"
                )
                for ref in unique_additional_refs:
                    print(f"  - {ref['law_name']} ì œ{ref['article_num']}ì¡°")

                # ì¶”ê°€ ì°¸ì¡° ì¡°í•­ì„ referenced_lawsì— í•©ì¹˜ê¸°
                referenced_laws.extend(unique_additional_refs)

                # ì¶”ê°€ ì°¸ì¡° ì¡°í•­ì˜ ë‚´ìš©ë„ ê°€ì ¸ì˜¤ê¸°
                additional_contents = await self.law_fetcher.fetch_law_articles_content(
                    unique_additional_refs
                )
                law_contents.extend(additional_contents)

        # 6. RAGìš© context ìƒì„± (í¬ë¡¤ë§+ë²•ë ¹ ë‚´ìš©)
        rag_context = all_text
        for law in law_contents:
            if law.get("content", {}).get("success"):
                c = law["content"]["content"].get("content", "")
                if c:
                    rag_context += f"\n\n--- ë²•ë ¹ ì¡°ë¬¸ ---\n\n{c}"

        # ë””ë²„ê·¸: í¬ë¡¤ë§ëœ ë‚´ìš© ì¶œë ¥
        print(f"\nğŸ” í¬ë¡¤ë§ëœ ë‚´ìš© (ì²˜ìŒ 500ì):\n{all_text[:500]}...")

        # 7. LLM ë‹µë³€ ìƒì„±
        llm_answer = None
        # ë²•ë ¹ ì¶”ì¶œ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ LLM ë‹µë³€ ì°¨ë‹¨
        # if self.openai_client and (direct_laws or referenced_laws):
        if self.openai_client:
            try:
                prompt = f"""
ì•„ë˜ëŠ” ë²•ë ¹ ë° ê´€ë ¨ ì¡°ë¬¸ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë²•ì  ê·¼ê±°ì™€ í•¨ê»˜ ëª…í™•í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”. ë‹µë³€ì€ ìµœëŒ€í•œ ë²•ë ¹ ë‚´ìš©ì„ ì¸ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

[ë²•ë ¹ ë° ì¡°ë¬¸]
{rag_context}

[ì§ˆë¬¸]
{query}

[ë‹µë³€]
"""
                model_name = self.openai_model or "gpt-3.5-turbo-16k"
                response = self.openai_client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt},
                    ],
                    temperature=0.2,
                    max_tokens=800,
                )
                llm_answer = (
                    response.choices[0].message.content.strip()
                    if response.choices[0].message.content
                    else None
                )
                # print(f"DEBUG: LLM ë‹µë³€: {llm_answer}")
            except Exception as e:
                print(f"LLM ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
                llm_answer = None

        return {
            "success": True,
            "search_query": query,
            "crawled_content": (
                all_text[:2000] + "..." if len(all_text) > 2000 else all_text
            ),
            # "extracted_laws": extracted_laws,
            # "direct_laws": direct_laws,
            # "referenced_laws": referenced_laws,
            "law_contents": law_contents,
            "llm_answer": llm_answer,
        }

    def get_law_domains(self) -> List[str]:
        """ë²•ë ¹ ê´€ë ¨ ë„ë©”ì¸ ëª©ë¡ ë°˜í™˜"""
        return [
            "law.go.kr",  # êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°
            # "casenote.kr",  # ì¼€ì´ìŠ¤ë…¸íŠ¸
            # "bigcase.ai",  # ë¹…ì¼€ì´ìŠ¤
            # "scourt.go.kr",  # ëŒ€ë²•ì›
            # "klaw.go.kr",  # í•œêµ­ë²•ì œì—°êµ¬ì›
        ]

    def get_news_domains(self) -> List[str]:
        """ë‰´ìŠ¤ ê´€ë ¨ ë„ë©”ì¸ ëª©ë¡ ë°˜í™˜"""
        return [
            "news.naver.com",  # ë„¤ì´ë²„ ë‰´ìŠ¤
            # "news.daum.net",  # ë‹¤ìŒ ë‰´ìŠ¤
            # "news.khan.co.kr",  # ê²½í–¥ì‹ ë¬¸
            # "chosun.com",  # ì¡°ì„ ì¼ë³´
            # "joongang.co.kr",  # ì¤‘ì•™ì¼ë³´
        ]

    def format_results(self, results: Dict[str, Any]) -> str:
        """ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…"""
        if not results.get("success"):
            return f"âŒ ì˜¤ë¥˜: {results.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"

        output = f"# ê²€ìƒ‰ ê²°ê³¼: '{results['search_query']}'\n\n"

        # ì¶”ì¶œëœ ë²•ë ¹ ìš”ì•½
        direct_laws = results.get("direct_laws", [])
        referenced_laws = results.get("referenced_laws", [])
        all_laws = results.get("extracted_laws", [])

        if all_laws:
            output += f"## ğŸ“‹ ë°œê²¬ëœ ë²•ë ¹ ({len(all_laws)}ê°œ)\n\n"

            # ì§ì ‘ ì–¸ê¸‰ëœ ë²•ë ¹
            if direct_laws:
                output += "### ì§ì ‘ ì–¸ê¸‰ëœ ë²•ë ¹\n\n"
                for i, law in enumerate(direct_laws, 1):
                    output += f"{i}. **{law['law_name']}** ì œ{law['article_num']}ì¡°\n"
                output += "\n"

            # ì°¸ì¡°ëœ ë²•ë ¹
            if referenced_laws:
                output += "### ì°¸ì¡°ëœ ë²•ë ¹\n\n"
                for i, law in enumerate(referenced_laws, 1):
                    ref_type = law.get("reference_type", "ì°¸ì¡°")
                    output += f"{i}. **{law['law_name']}** ì œ{law['article_num']}ì¡° ({ref_type})\n"
                output += "\n"

        # # ì¡°ë¬¸ ë‚´ìš©
        # law_contents = results.get("law_contents", [])
        # if law_contents:
        #     output += "## ğŸ“– ì¡°ë¬¸ ë‚´ìš©\n\n"
        #     for i, result in enumerate(law_contents, 1):
        #         original = result["original_article"]
        #         content = result["content"]

        #         print(f"DEBUG: top level content: {content}")

        #         output += (
        #             f"### {i}. {original['law_name']} ì œ{original['article_num']}ì¡°\n\n"
        #         )

        #         if content.get("success"):
        #             content_data = content.get("content", {})
        #             title = content_data.get("title", "ì œëª© ì—†ìŒ")
        #             law_content = content_data.get("content", "ë‚´ìš© ì—†ìŒ")

        #             output += f"**ì œëª©**: {title}\n\n"
        #             output += f"**ë‚´ìš©**:\n{law_content}\n\n"
        #         else:
        #             output += (
        #                 f"âŒ **ì˜¤ë¥˜**: {content.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}\n\n"
        #             )

        #         output += "---\n\n"
        # else:
        #     output += "## ğŸ“– ì¡°ë¬¸ ë‚´ìš©\n\nâŒ ì¡°ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"

        # LLM ë‹µë³€ (ë²•ë ¹ì´ ì¶”ì¶œëœ ê²½ìš°ì—ë§Œ ì¶œë ¥)
        llm_answer = results.get("llm_answer")
        direct_laws = results.get("direct_laws", [])
        referenced_laws = results.get("referenced_laws", [])

        if llm_answer and (direct_laws or referenced_laws):
            output += "## ğŸ¤– LLM ë‹µë³€\n\n"
            output += llm_answer.strip() + "\n\n"
        elif llm_answer:
            output += "## âš ï¸  ì£¼ì˜\n\n"
            output += (
                "ë²•ë ¹ì„ ì°¾ì§€ ëª»í–ˆì§€ë§Œ LLM ë‹µë³€ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì˜¤ë¥˜ì…ë‹ˆë‹¤.\n\n"
            )
        else:
            output += "## âš ï¸  ê²°ê³¼\n\n"
            output += "ê´€ë ¨ ë²•ë ¹ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”.\n\n"

        return output


async def main():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("=== í†µí•© ë²•ë ¹ ê²€ìƒ‰ ì‹œìŠ¤í…œ ===\n")

    # ì‚¬ìš©ì ì…ë ¥ ì‹œë®¬ë ˆì´ì…˜
    # user_query = "ê±´ì¶•ë²•ì—ì„œ ê²½ë¯¸í•œ ì‚¬í•­ì˜ ë³€ê²½ì´ë€?"
    user_query = "ê±´ì„¤ì‚¬ì—…ê´€ë¦¬ê¸°ìˆ ì¸ì˜ í˜„ì¥ ì² ìˆ˜ í†µë³´ì— ê´€ë ¨ëœ ì¡°í•­ì„ ì•Œë ¤ì£¼ì„¸ìš”."

    print(f"ì‚¬ìš©ì ì§ˆë¬¸: {user_query}\n")

    # í†µí•© ê²€ìƒ‰ ì‹¤í–‰
    searcher = LawSearchIntegrated()

    # ë²•ë ¹ ì‚¬ì´íŠ¸ì—ì„œ ê²€ìƒ‰
    law_domains = searcher.get_law_domains()
    # results = await searcher.crawl_and_extract_laws(user_query, law_domains, 5)
    results = await searcher.crawl_and_extract_laws(user_query, None, 5)

    # ê²°ê³¼ ì¶œë ¥
    formatted_output = searcher.format_results(results)
    print(formatted_output)


if __name__ == "__main__":
    asyncio.run(main())
